{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":227139,"status":"ok","timestamp":1759309964029,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"xTcWtGOYX9V4","outputId":"c014fa1f-4930-479c-c9e0-44939f5cafb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.8.0+cu126\n","Uninstalling torch-2.8.0+cu126:\n","  Successfully uninstalled torch-2.8.0+cu126\n","\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: torchvision 0.23.0+cu126\n","Uninstalling torchvision-0.23.0+cu126:\n","  Successfully uninstalled torchvision-0.23.0+cu126\n","Found existing installation: torchaudio 2.8.0+cu126\n","Uninstalling torchaudio-2.8.0+cu126:\n","  Successfully uninstalled torchaudio-2.8.0+cu126\n","Collecting numpy==1.26.4\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.8.4 requires torch<2.9,>=1.10, which is not installed.\n","fastai 2.8.4 requires torchvision>=0.11, which is not installed.\n","sentence-transformers 5.1.0 requires torch>=1.11.0, which is not installed.\n","accelerate 1.10.1 requires torch>=2.0.0, which is not installed.\n","peft 0.17.1 requires torch>=1.13.0, which is not installed.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"e6c2494a4c1648f4a758b9b3ed6fbffb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting torch==2.2.0\n","  Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision==0.17.0\n","  Downloading torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting torchaudio==2.2.0\n","  Downloading torchaudio-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (4.15.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch==2.2.0)\n","  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.32.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (11.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.85)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n","Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.4/755.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.17.0-cp312-cp312-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.2.0-cp312-cp312-manylinux1_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.4.0\n","    Uninstalling triton-3.4.0:\n","      Successfully uninstalled triton-3.4.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchaudio-2.2.0 torchvision-0.17.0 triton-2.2.0\n","Collecting torchtext==0.17.2\n","  Downloading torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (2.32.4)\n","Collecting torch==2.2.2 (from torchtext==0.17.2)\n","  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.15.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.6.85)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2025.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n","Downloading torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.0\n","    Uninstalling torch-2.2.0:\n","      Successfully uninstalled torch-2.2.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\n","torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.2.2 torchtext-0.17.2\n","Collecting albumentations==1.3.1\n","  Downloading albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (1.16.2)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (0.25.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (6.0.2)\n","Collecting qudida>=0.0.4 (from albumentations==1.3.1)\n","  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (4.12.0.88)\n","Collecting numpy>=1.11.1 (from albumentations==1.3.1)\n","  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (1.6.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (4.15.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2025.9.9)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (3.6.0)\n","Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n","Installing collected packages: numpy, qudida, albumentations\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 2.0.8\n","    Uninstalling albumentations-2.0.8:\n","      Successfully uninstalled albumentations-2.0.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed albumentations-1.3.1 numpy-2.2.6 qudida-0.0.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"01c0d1b3b33d483585d5796e0da097ce"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting timm==0.9.2\n","  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from timm==0.9.2) (2.2.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.9.2) (0.17.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==0.9.2) (6.0.2)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from timm==0.9.2) (0.35.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm==0.9.2) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (4.15.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (12.1.105)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm==0.9.2) (12.6.85)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.2) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.2) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.2) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.2) (1.1.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.9.2) (2.2.6)\n","Collecting torch>=1.7 (from timm==0.9.2)\n","  Using cached torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.9.2) (11.3.0)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.2) (2.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->timm==0.9.2) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.2) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.7->timm==0.9.2) (1.3.0)\n","Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n","Installing collected packages: torch, timm\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.2.2\n","    Uninstalling torch-2.2.2:\n","      Successfully uninstalled torch-2.2.2\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.19\n","    Uninstalling timm-1.0.19:\n","      Successfully uninstalled timm-1.0.19\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.17.2 requires torch==2.2.2, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed timm-0.9.2 torch-2.2.0\n","Collecting opencv-python==4.8.0.74\n","  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from opencv-python==4.8.0.74) (2.2.6)\n","Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.12.0.88\n","    Uninstalling opencv-python-4.12.0.88:\n","      Successfully uninstalled opencv-python-4.12.0.88\n","Successfully installed opencv-python-4.8.0.74\n","Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n","Collecting gTTS\n","  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n","Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n","Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n","Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.6)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n","Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gTTS) (2.32.4)\n","Collecting click<8.2,>=7.1 (from gTTS)\n","  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n","Downloading click-8.1.8-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: click, gTTS\n","  Attempting uninstall: click\n","    Found existing installation: click 8.2.1\n","    Uninstalling click-8.2.1:\n","      Successfully uninstalled click-8.2.1\n","Successfully installed click-8.1.8 gTTS-2.5.4\n"]}],"source":["!pip uninstall torch torchtext torchvision torchaudio -y\n","!pip install numpy==1.26.4\n","!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n","!pip install torchtext==0.17.2\n","!pip install albumentations==1.3.1\n","!pip install timm==0.9.2\n","!pip install opencv-python==4.8.0.74\n","!pip install gradio gTTS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811},"executionInfo":{"elapsed":8807,"status":"ok","timestamp":1759310073317,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"Fn9MYpWYYPaP","outputId":"228e2517-7f7a-479b-83cc-2bd3db272918"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.12/dist-packages (1.3.1)\n","Requirement already satisfied: opencv-python==4.8.0.74 in /usr/local/lib/python3.12/dist-packages (4.8.0.74)\n","Collecting numpy==1.26.4\n","  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (1.16.2)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (0.25.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (6.0.2)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1) (4.12.0.88)\n","INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python-headless>=4.1.1 (from albumentations==1.3.1)\n","  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (1.6.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (4.15.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2025.9.9)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (3.6.0)\n","Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, opencv-python-headless\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.2.6\n","    Uninstalling numpy-2.2.6:\n","      Successfully uninstalled numpy-2.2.6\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.12.0.88\n","    Uninstalling opencv-python-headless-4.12.0.88:\n","      Successfully uninstalled opencv-python-headless-4.12.0.88\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.17.2 requires torch==2.2.2, but you have torch 2.2.0 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4 opencv-python-headless-4.11.0.86\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"df239c31590d46ec94d8bf118c8e62bc"}},"metadata":{}}],"source":["!pip install albumentations==1.3.1 opencv-python==4.8.0.74 numpy==1.26.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1701,"status":"ok","timestamp":1759310089742,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"F9JYrnbbYQ2l","outputId":"1b08cc39-5ea2-4541-a4ef-9d1e408e5f14"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.0+cu121\n","0.17.2+cpu\n"]}],"source":["import torch\n","import torchtext\n","print(torch.__version__)\n","print(torchtext.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9518,"status":"ok","timestamp":1759310101978,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"30kHq23WYV-N","outputId":"09cc795e-4675-45e9-b91c-957af17a5ceb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":2}],"source":["import torchtext\n","from torchtext.data.utils import get_tokenizer\n","import torch\n","import torch.nn as nn\n","import albumentations as alb\n","import timm\n","import cv2\n","import pandas as pd\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import Dataset,DataLoader\n","from collections import Counter\n","\n","device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxCuFa5lYV6m"},"outputs":[],"source":["class ImageCaptioner(nn.Module):\n","    def __init__(self,context_length,vocabulary_size,num_blocks,model_dim,num_heads,prob):\n","        super().__init__()\n","        self.cnn_encoder = timm.create_model('efficientnet_b0', pretrained=True)\n","        test_image=torch.zeros(1,3,224,224) #batch_size,channels,height,width\n","        with torch.no_grad():\n","          cnn_output=self.cnn_encoder(test_image) #B->1, in_features\n","        in_features=cnn_output.shape[1]\n","        self.project=nn.Linear(in_features,model_dim)\n","        #here, we we will mot use pretrained model instead we define it and train it ourselves\n","        self.word_embeddings=nn.Embedding(vocabulary_size,model_dim)\n","        self.pos_embeddings=nn.Embedding(context_length,model_dim)\n","\n","        block=nn.TransformerDecoderLayer(model_dim,num_heads,2*model_dim,dropout=prob,batch_first=True,norm_first=True)\n","        self.blocks=nn.TransformerDecoder(block,num_blocks)\n","        #the linear layer at the end of the transformer\n","        self.vocab_projection=nn.Linear(model_dim,vocabulary_size)\n","    def forward(self,images,true_labels):\n","        #pass the encoded images into decoder\n","        #get the embeddings and add them\n","        #pass the captions into embedding layer\n","        tok_embedded=self.word_embeddings(true_labels)\n","        B,T=true_labels.shape\n","        positions = torch.arange(T).to(device)\n","        pos_embedded = self.pos_embeddings(positions)\n","        total_embeddings=tok_embedded+pos_embedded #input to tranformer block\n","\n","        with torch.no_grad():\n","          encoded_image=self.project(self.cnn_encoder(images).view(B,-1)) #B,something\n","\n","        #captions->B,T,C\n","        #Images->B,C (#add 1 o match the shape of captions(B,1,C))\n","        img_for_attention=torch.unsqueeze(encoded_image,1)\n","        #Memory=Additional Context\n","        #Casual/Subsequent Mask -> Future tokens are masked out\n","        attention_mask=nn.Transformer.generate_square_subsequent_mask(T).to(device)\n","        block_output=self.blocks(total_embeddings,img_for_attention,tgt_mask=attention_mask)\n","\n","        vocabulary_vector=self.vocab_projection(block_output) #B,T,V\n","\n","        return vocabulary_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130728,"status":"ok","timestamp":1759310239716,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"9T3GbDgDYV4I","outputId":"69cebd43-a040-40e5-dbe4-33bb33829176"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Flicker8k\n","['Images', 'final_training_loss_history.csv', 'captions.txt', 'weights.pt', '.gradio', 'Image_Captioner.ipynb']\n","8091\n","40455\n"]}],"source":["from google.colab import drive\n","from google.colab.patches import cv2_imshow\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/Flicker8k\"\n","\n","# Path to your folder\n","folder_path = \"/content/drive/MyDrive/Flicker8k\"\n","\n","captions_filename='captions.txt'\n","missing='2258277193_586949ec62.jpg'\n","\n","#Image, Caption\n","\n","with open(captions_filename) as captions:\n","  lines=captions.readlines()\n","\n","get_captions={}\n","all_captions=[] #5*len(get_captions)\n","\n","#split '.jpg'\n","\n","for caption in lines:\n","  data=caption.rstrip('\\n').split('.jpg,')\n","  # Check if data has at least two elements before accessing data[1]\n","  img_name=data[0]+'.jpg'\n","  if img_name == missing:\n","    continue\n","  caption_list=get_captions.get(img_name,[])\n","  caption_list.append(data[1])\n","  get_captions[img_name]=caption_list\n","  all_captions.append(data[1])\n","\n","import os\n","print(os.listdir('/content/drive/MyDrive/Flicker8k'))\n","\n","print(len(get_captions))\n","print(len(all_captions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMjM8CLJYV1p"},"outputs":[],"source":["#Dataset class stores all the training data, __init__,__len__,__get_item__(idx)\n","\n","#grab the random batch of overall training data at every iteration\n","#DataFrame(pandas)->table of rows and columns\n","\n","df=pd.DataFrame(columns=['filename','caption'])\n","df['filename']=get_captions.keys()\n","df['caption']=df['filename'].map(lambda filename:get_captions[filename])\n","\n","vocab_frequency=Counter()\n","word_tokenizer=get_tokenizer('basic_english')\n","\n","for cap in all_captions:\n","    vocab_frequency.update(word_tokenizer(cap))\n","\n","vocabulary=torchtext.vocab.vocab(vocab_frequency)\n","vocabulary.insert_token('<UNKNOWN>',0) #it will kick out the words that appear super low in our dataset\n","vocabulary.insert_token('<PAD>',1)\n","vocabulary.insert_token('<START>',2)\n","vocabulary.insert_token('<END>',3)\n","vocabulary.set_default_index(0)\n","\n","context_length=20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WrXRArIYVy7"},"outputs":[],"source":["class ImageCaptioningDataset(Dataset):\n","    def __init__(self,split):\n","        self.df=df\n","        self.img_size=224\n","        transformation_list=[alb.Resize(self.img_size,self.img_size)]\n","        if split == 'training':\n","            transformation_list.append(alb.HorizontalFlip())\n","            transformation_list.append(alb.ColorJitter())\n","        transformation_list.append(alb.Normalize())\n","        transformation_list.append(ToTensorV2())\n","\n","        self.transformations=alb.Compose(transformation_list)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self,idx): #idx->[0,len(dataset)]\n","        #Image,caption\n","        image_filename,captions=self.df.iloc[idx]\n","        actual_image=cv2.cvtColor(cv2.imread('Images/'+ image_filename),cv2.COLOR_BGR2RGB)\n","        cv2_imshow(actual_image)\n","        transformed_img=self.transformations(image=actual_image)['image']\n","        encoded_captions= [] #a list of list of integers\n","        for i,cap in enumerate(captions):\n","            splitted=word_tokenizer(cap)\n","            #integers are words here\n","            integers=[vocabulary[word] for word in splitted]\n","            integers=[2] + integers +[3] #2,3 are <START>,<END> tokens\n","\n","            if len(integers) <= context_length:\n","                pads_to_add=context_length-len(integers)\n","                integers+=[1]*pads_to_add #padding token is 1\n","            else:\n","                integers=integers[:context_length-1] + [3]\n","\n","            encoded_captions.append(torch.tensor(integers,dtype=torch.long))\n","\n","        random_idx=torch.randint(5,(1,)).item() #when the image came we are randomly picking any of the 5 captions that we have\n","        return transformed_img,encoded_captions[random_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["7b5b1f20aaec4e00b2e97f2c8495c49c","0434c872bc244a628e3d954b482ecb55","a3aaf1922dfb40cb82deba6bd4ddbe57","6b90fd20be5e4ff8af4637f8060f6e85","09992b8031f843d78afc072aa4ff5fe5","c62624795bc940239bfd5f2cf7846e7d","d490cd79f9014e57856cba1927bddfd4","59bff532d40a47fa8fce87dc560f982a","dd5a18099d3a4ca7a1741a32dc1b2f72","6c0df289bb2e4a5eb96f489d8f4c7988","26ab1c3adf554161b59992d88910f0da"]},"executionInfo":{"elapsed":2387,"status":"ok","timestamp":1759310290506,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"ULlanqq9YVwH","outputId":"e2768e4b-0b0b-4493-9265-243d2188d9ad"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5b1f20aaec4e00b2e97f2c8495c49c"}},"metadata":{}}],"source":["training_dataset = ImageCaptioningDataset('training')\n","training_data = DataLoader(training_dataset, batch_size=32, shuffle=True)\n","\n","context_length = 20\n","vocabulary_size = len(vocabulary)\n","num_blocks = 6\n","model_dim = 512\n","num_heads = 16\n","prob = 0.5\n","\n","model = ImageCaptioner(context_length, vocabulary_size, num_blocks, model_dim, num_heads, prob).to(device)\n","\n","for layer in model.cnn_encoder.parameters():\n","    layer.requires_grad = False\n","\n","loss_function = nn.CrossEntropyLoss(ignore_index=vocabulary['<PAD>'])\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3467566,"status":"ok","timestamp":1759118898614,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"Ks0LJW_Mkb2N","outputId":"cc01efb4-0b37-48a0-f746-6151ab082bf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss: 11.087166786193848\n","Epoch: 0, Loss: 6.157675743103027\n","Epoch: 0, Loss: 4.470897674560547\n","Epoch: 1, Loss: 4.058785438537598\n","Epoch: 1, Loss: 3.3339686393737793\n","Epoch: 1, Loss: 2.931006908416748\n","Epoch: 2, Loss: 2.915435552597046\n","Epoch: 2, Loss: 2.7991888523101807\n","Epoch: 3, Loss: 2.4908628463745117\n","Epoch: 3, Loss: 2.714989423751831\n","Epoch: 3, Loss: 2.3377065658569336\n","Epoch: 4, Loss: 2.083366632461548\n","Epoch: 4, Loss: 2.048682451248169\n","Epoch: 5, Loss: 1.8271465301513672\n","Epoch: 5, Loss: 2.218033790588379\n","Epoch: 5, Loss: 1.7677762508392334\n","Epoch: 6, Loss: 1.9576985836029053\n","Epoch: 6, Loss: 1.6872563362121582\n","Epoch: 7, Loss: 1.847063422203064\n","Epoch: 7, Loss: 1.597673773765564\n","Epoch: 7, Loss: 1.54435133934021\n","Epoch: 8, Loss: 1.9394001960754395\n","Epoch: 8, Loss: 1.5236948728561401\n","Epoch: 9, Loss: 1.2693419456481934\n","Epoch: 9, Loss: 1.1137865781784058\n","Epoch: 9, Loss: 1.477687120437622\n","Epoch: 10, Loss: 1.3897596597671509\n","Epoch: 10, Loss: 1.298701524734497\n","Epoch: 11, Loss: 1.3369178771972656\n","Epoch: 11, Loss: 1.1621406078338623\n","Epoch: 11, Loss: 1.034082055091858\n","Epoch: 12, Loss: 1.2518153190612793\n","Epoch: 12, Loss: 0.9624465107917786\n","Epoch: 13, Loss: 0.953216016292572\n","Epoch: 13, Loss: 1.1000330448150635\n","Epoch: 13, Loss: 1.1547794342041016\n","Epoch: 14, Loss: 1.0661965608596802\n","Epoch: 14, Loss: 0.8584538698196411\n","Epoch: 15, Loss: 0.9149393439292908\n","Epoch: 15, Loss: 0.8587766885757446\n","Epoch: 15, Loss: 0.7750239372253418\n","Epoch: 16, Loss: 0.9716346263885498\n","Epoch: 16, Loss: 0.6969062089920044\n","Epoch: 16, Loss: 0.8846682906150818\n","Epoch: 17, Loss: 0.6961291432380676\n","Epoch: 17, Loss: 0.8672308325767517\n","Epoch: 18, Loss: 0.9102621674537659\n","Epoch: 18, Loss: 0.8218756914138794\n","Epoch: 18, Loss: 0.7745260000228882\n","Epoch: 19, Loss: 0.6891475915908813\n","Epoch: 19, Loss: 0.5002652406692505\n","Epoch: 20, Loss: 0.7190889716148376\n","Epoch: 20, Loss: 0.7663262486457825\n","Epoch: 20, Loss: 0.8964598774909973\n","Epoch: 21, Loss: 0.4841623306274414\n","Epoch: 21, Loss: 0.7024327516555786\n","Epoch: 22, Loss: 0.6788814067840576\n","Epoch: 22, Loss: 0.6542582511901855\n","Epoch: 22, Loss: 0.5597040057182312\n","Epoch: 23, Loss: 0.5159876942634583\n","Epoch: 23, Loss: 0.46905022859573364\n","Epoch: 24, Loss: 0.5640596747398376\n","Epoch: 24, Loss: 0.7769172191619873\n","Epoch: 24, Loss: 0.5229994058609009\n","Epoch: 25, Loss: 0.6147537231445312\n","Epoch: 25, Loss: 0.40539219975471497\n","Epoch: 26, Loss: 0.6349520087242126\n","Epoch: 26, Loss: 0.6200547814369202\n","Epoch: 26, Loss: 0.574954092502594\n","Epoch: 27, Loss: 0.561079740524292\n","Epoch: 27, Loss: 0.49141600728034973\n","Epoch: 28, Loss: 0.6240969896316528\n","Epoch: 28, Loss: 0.48631107807159424\n","Epoch: 28, Loss: 0.6468267440795898\n","Epoch: 29, Loss: 0.6917904019355774\n","Epoch: 29, Loss: 0.3380948007106781\n","Epoch: 30, Loss: 0.47387900948524475\n","Epoch: 30, Loss: 0.3501122295856476\n","Epoch: 30, Loss: 0.6164442896842957\n","Epoch: 31, Loss: 0.22451555728912354\n","Epoch: 31, Loss: 0.3512970805168152\n","Epoch: 32, Loss: 0.44859978556632996\n","Epoch: 32, Loss: 0.40284648537635803\n","Epoch: 32, Loss: 0.5798299312591553\n","Epoch: 33, Loss: 0.26701387763023376\n","Epoch: 33, Loss: 0.5227110385894775\n","Epoch: 33, Loss: 0.5904723405838013\n","Epoch: 34, Loss: 0.3913693130016327\n","Epoch: 34, Loss: 0.5313878059387207\n","Epoch: 35, Loss: 0.2747341990470886\n","Epoch: 35, Loss: 0.49560901522636414\n","Epoch: 35, Loss: 0.38015639781951904\n","Epoch: 36, Loss: 0.2905752658843994\n","Epoch: 36, Loss: 0.43631449341773987\n","Epoch: 37, Loss: 0.3412679135799408\n","Epoch: 37, Loss: 0.322769433259964\n","Epoch: 37, Loss: 0.602993369102478\n","Epoch: 38, Loss: 0.43518179655075073\n","Epoch: 38, Loss: 0.31012099981307983\n","Epoch: 39, Loss: 0.39505717158317566\n","Epoch: 39, Loss: 0.2889271080493927\n","Epoch: 39, Loss: 0.5247768759727478\n","Epoch: 40, Loss: 0.4810091257095337\n","Epoch: 40, Loss: 0.27849775552749634\n","Epoch: 41, Loss: 0.24172158539295197\n","Epoch: 41, Loss: 0.2977631390094757\n","Epoch: 41, Loss: 0.3142758309841156\n","Epoch: 42, Loss: 0.3089057207107544\n","Epoch: 42, Loss: 0.35250124335289\n","Epoch: 43, Loss: 0.2648909389972687\n","Epoch: 43, Loss: 0.43789780139923096\n","Epoch: 43, Loss: 0.40042319893836975\n","Epoch: 44, Loss: 0.26158279180526733\n","Epoch: 44, Loss: 0.36593472957611084\n","Epoch: 45, Loss: 0.32888156175613403\n","Epoch: 45, Loss: 0.29529035091400146\n","Epoch: 45, Loss: 0.3740752339363098\n","Epoch: 46, Loss: 0.22246859967708588\n","Epoch: 46, Loss: 0.23068413138389587\n","Epoch: 47, Loss: 0.3075634837150574\n","Epoch: 47, Loss: 0.2016306072473526\n","Epoch: 47, Loss: 0.2635578513145447\n","Epoch: 48, Loss: 0.20113803446292877\n","Epoch: 48, Loss: 0.18166609108448029\n","Epoch: 49, Loss: 0.2896040976047516\n","Epoch: 49, Loss: 0.19731128215789795\n","Epoch: 49, Loss: 0.2976711392402649\n","Epoch: 50, Loss: 0.22470681369304657\n","Epoch: 50, Loss: 0.3064189851284027\n","Epoch: 50, Loss: 0.24443897604942322\n","Epoch: 51, Loss: 0.20622730255126953\n","Epoch: 51, Loss: 0.2308875322341919\n","Epoch: 52, Loss: 0.20989219844341278\n","Epoch: 52, Loss: 0.3285227417945862\n","Epoch: 52, Loss: 0.28992101550102234\n","Epoch: 53, Loss: 0.3142721354961395\n","Epoch: 53, Loss: 0.29268157482147217\n","Epoch: 54, Loss: 0.2362109273672104\n","Epoch: 54, Loss: 0.14746828377246857\n","Epoch: 54, Loss: 0.27258825302124023\n","Epoch: 55, Loss: 0.15565082430839539\n","Epoch: 55, Loss: 0.15963131189346313\n","Epoch: 56, Loss: 0.21400955319404602\n","Epoch: 56, Loss: 0.1716531217098236\n","Epoch: 56, Loss: 0.23736058175563812\n","Epoch: 57, Loss: 0.11068324744701385\n","Epoch: 57, Loss: 0.18261279165744781\n","Epoch: 58, Loss: 0.2684912383556366\n","Epoch: 58, Loss: 0.3179031312465668\n","Epoch: 58, Loss: 0.18386121094226837\n","Epoch: 59, Loss: 0.15334749221801758\n","Epoch: 59, Loss: 0.19882804155349731\n","Epoch: 60, Loss: 0.13802510499954224\n","Epoch: 60, Loss: 0.24855507910251617\n","Epoch: 60, Loss: 0.11882425844669342\n","Epoch: 61, Loss: 0.2045414298772812\n","Epoch: 61, Loss: 0.1000182256102562\n","Epoch: 62, Loss: 0.1860678493976593\n","Epoch: 62, Loss: 0.17205004394054413\n","Epoch: 62, Loss: 0.23713970184326172\n","Epoch: 63, Loss: 0.19177751243114471\n","Epoch: 63, Loss: 0.168614462018013\n","Epoch: 64, Loss: 0.09029658138751984\n","Epoch: 64, Loss: 0.10128641873598099\n","Epoch: 64, Loss: 0.0955856665968895\n","Epoch: 65, Loss: 0.19809839129447937\n","Epoch: 65, Loss: 0.19885118305683136\n","Epoch: 66, Loss: 0.14886681735515594\n","Epoch: 66, Loss: 0.13799303770065308\n","Epoch: 66, Loss: 0.16866810619831085\n","Epoch: 67, Loss: 0.1571175903081894\n","Epoch: 67, Loss: 0.1741882711648941\n","Epoch: 67, Loss: 0.11516822874546051\n","Epoch: 68, Loss: 0.07107802480459213\n","Epoch: 68, Loss: 0.20873074233531952\n","Epoch: 69, Loss: 0.05451546609401703\n","Epoch: 69, Loss: 0.07819350063800812\n","Epoch: 69, Loss: 0.10949748754501343\n","Epoch: 70, Loss: 0.14483456313610077\n","Epoch: 70, Loss: 0.21015062928199768\n","Epoch: 71, Loss: 0.13886037468910217\n","Epoch: 71, Loss: 0.1882251650094986\n","Epoch: 71, Loss: 0.14438416063785553\n","Epoch: 72, Loss: 0.08284542709589005\n","Epoch: 72, Loss: 0.08922984451055527\n","Epoch: 73, Loss: 0.14162017405033112\n","Epoch: 73, Loss: 0.1423436850309372\n","Epoch: 73, Loss: 0.1091957613825798\n","Epoch: 74, Loss: 0.21824981272220612\n","Epoch: 74, Loss: 0.1190880537033081\n","Epoch: 75, Loss: 0.1379285305738449\n","Epoch: 75, Loss: 0.10926125198602676\n","Epoch: 75, Loss: 0.11186101287603378\n","Epoch: 76, Loss: 0.11495489627122879\n","Epoch: 76, Loss: 0.08616945892572403\n","Epoch: 77, Loss: 0.06788842380046844\n","Epoch: 77, Loss: 0.1540261209011078\n","Epoch: 77, Loss: 0.06718788295984268\n","Epoch: 78, Loss: 0.08495534956455231\n","Epoch: 78, Loss: 0.1239437535405159\n","Epoch: 79, Loss: 0.08655164390802383\n","Epoch: 79, Loss: 0.09497985988855362\n","Epoch: 79, Loss: 0.09765461832284927\n","Epoch: 80, Loss: 0.11505978554487228\n","Epoch: 80, Loss: 0.04644043371081352\n","Epoch: 81, Loss: 0.0897752195596695\n","Epoch: 81, Loss: 0.08842337876558304\n","Epoch: 81, Loss: 0.08364138007164001\n","Epoch: 82, Loss: 0.058182571083307266\n","Epoch: 82, Loss: 0.17239674925804138\n","Epoch: 83, Loss: 0.12447383999824524\n","Epoch: 83, Loss: 0.06367573887109756\n","Epoch: 83, Loss: 0.08232971280813217\n","Epoch: 84, Loss: 0.11462290585041046\n","Epoch: 84, Loss: 0.09912944585084915\n","Epoch: 84, Loss: 0.11497388780117035\n","Epoch: 85, Loss: 0.08571432530879974\n","Epoch: 85, Loss: 0.057180535048246384\n","Epoch: 86, Loss: 0.027588848024606705\n","Epoch: 86, Loss: 0.06797779351472855\n","Epoch: 86, Loss: 0.08951413631439209\n","Epoch: 87, Loss: 0.09032776951789856\n","Epoch: 87, Loss: 0.0844707116484642\n","Epoch: 88, Loss: 0.1258877068758011\n","Epoch: 88, Loss: 0.13069559633731842\n","Epoch: 88, Loss: 0.04210195317864418\n","Epoch: 89, Loss: 0.12805384397506714\n","Epoch: 89, Loss: 0.14038397371768951\n","Epoch: 90, Loss: 0.06832931935787201\n","Epoch: 90, Loss: 0.05205850675702095\n","Epoch: 90, Loss: 0.03627341240644455\n","Epoch: 91, Loss: 0.07739930599927902\n","Epoch: 91, Loss: 0.1132458820939064\n","Epoch: 92, Loss: 0.05838584527373314\n","Epoch: 92, Loss: 0.11003924161195755\n","Epoch: 92, Loss: 0.03982846438884735\n","Epoch: 93, Loss: 0.126725435256958\n","Epoch: 93, Loss: 0.09385009109973907\n","Epoch: 94, Loss: 0.0765450969338417\n","Epoch: 94, Loss: 0.08835496753454208\n","Epoch: 94, Loss: 0.12753991782665253\n","Epoch: 95, Loss: 0.08220040798187256\n","Epoch: 95, Loss: 0.1071479544043541\n","Epoch: 96, Loss: 0.1165129616856575\n","Epoch: 96, Loss: 0.028428904712200165\n","Epoch: 96, Loss: 0.05073975771665573\n","Epoch: 97, Loss: 0.02734176069498062\n","Epoch: 97, Loss: 0.09008404612541199\n","Epoch: 98, Loss: 0.1180684044957161\n","Epoch: 98, Loss: 0.09915248304605484\n","Epoch: 98, Loss: 0.0877341702580452\n","Epoch: 99, Loss: 0.09277850389480591\n","Epoch: 99, Loss: 0.12116431444883347\n"]}],"source":["num_epochs=100\n","num_iterations=0\n","\n","for epoch in range(num_epochs):\n","  for images,captions in training_data:\n","    images,captions= images.to(device),captions.to(device)\n","    B,T=captions.shape\n","    #images->B,3,224,224,captions->B,T\n","    model_prediction=model(images,captions) #B,T,V\n","    model_prediction=model_prediction.view(B*T,vocabulary_size) # Corrected V to vocabulary_size\n","    loss=loss_function(model_prediction,captions.view(B*T))\n","    loss.backward() #calculate the gradients\n","    nn.utils.clip_grad_norm_(model.parameters(),2.0) #help prevent exploding gradient problem\n","    optimizer.step() #use the gradients to update the model parameters\n","    optimizer.zero_grad()\n","    if num_iterations % 100 == 0:\n","      print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n","    num_iterations+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8_678e72qpVT"},"outputs":[],"source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Flicker8k/weights.pt')\n","from google.colab import files\n","# files.download('weights.pt') # This line is no longer needed as we are saving directly to Drive"]},{"cell_type":"code","source":["import pickle\n","with open(\"/content/drive/MyDrive/Flicker8k/vocab.pkl\", \"wb\") as f:\n","    pickle.dump(vocabulary, f)"],"metadata":{"id":"kwn_EnnCPZl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8875,"status":"ok","timestamp":1759310306895,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"},"user_tz":-330},"id":"fDMEOs0wq-9t","outputId":"74e01d4c-b3b6-4f88-c812-19d555ece627"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":8}],"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/Flicker8k/weights.pt'))"]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Flicker8k/vocab.pkl\", \"rb\") as f:\n","    vocab = pickle.load(f)\n","integer_to_word = vocab.get_itos()"],"metadata":{"id":"ZuKyQgiHPY2g","executionInfo":{"status":"ok","timestamp":1759313273655,"user_tz":-330,"elapsed":78,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","execution_count":29,"metadata":{"id":"8821RiqvHN1X","executionInfo":{"status":"ok","timestamp":1759313276459,"user_tz":-330,"elapsed":6,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}}},"outputs":[],"source":["class ImageCaptioningDataset(Dataset):\n","    def __init__(self,split):\n","        self.df=df\n","        self.img_size=224\n","        transformation_list=[alb.Resize(self.img_size,self.img_size)]\n","        if split == 'training':\n","            transformation_list.append(alb.HorizontalFlip())\n","            transformation_list.append(alb.ColorJitter())\n","        transformation_list.append(alb.Normalize())\n","        transformation_list.append(ToTensorV2())\n","\n","        self.transformations=alb.Compose(transformation_list)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self,idx): #idx->[0,len(dataset)]\n","        #Image,caption\n","        image_filename,captions=self.df.iloc[idx]\n","        actual_image=cv2.cvtColor(cv2.imread('Images/'+ image_filename),cv2.COLOR_BGR2RGB)\n","        cv2_imshow(actual_image)\n","        transformed_img=self.transformations(image=actual_image)['image']\n","        encoded_captions= [] #a list of list of integers\n","        for i,cap in enumerate(captions):\n","            splitted=word_tokenizer(cap)\n","            #integers are words here\n","            integers=[vocabulary[word] for word in splitted]\n","            integers=[2] + integers +[3] #2,3 are <START>,<END> tokens\n","\n","            if len(integers) <= context_length:\n","                pads_to_add=context_length-len(integers)\n","                integers+=[1]*pads_to_add #padding token is 1\n","            else:\n","                integers=integers[:context_length-1] + [3]\n","\n","            encoded_captions.append(torch.tensor(integers,dtype=torch.long))\n","\n","        random_idx=torch.randint(5,(1,)).item() #when the image came we are randomly picking any of the 5 captions that we have\n","        return transformed_img,encoded_captions[random_idx]"]},{"cell_type":"code","source":["training_dataset=ImageCaptioningDataset('training')\n","training_data=DataLoader(training_dataset,batch_size=1,shuffle=True)"],"metadata":{"id":"p-LXGJBwTYbP","executionInfo":{"status":"ok","timestamp":1759313281315,"user_tz":-330,"elapsed":90,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_towe83W2oGlJyjMxlPBiMScyGHTH-CD"},"id":"ABdL_d8prwzi","outputId":"4d6fc896-a340-4685-ce21-cf0e5eec180d","executionInfo":{"status":"ok","timestamp":1759313467145,"user_tz":-330,"elapsed":5150,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["integer_to_word=vocabulary.get_itos()\n","num_epochs=1\n","num_iterations=0\n","max_images_to_display = 5 # Limit to displaying only the first 5 images\n","\n","for epoch in range(num_epochs):\n","  for x,y in training_data:\n","    #x is batch of images and y is corresponding captions/labels\n","    x,y=x.to(device),y.to(device)\n","    prediction=model(x,y) #B,T,vocab_size\n","    _,indices=torch.max(prediction,dim=-1)\n","    first_caption=indices[0] #T\n","    sentence=[]\n","    for id in first_caption:\n","      sentence.append(integer_to_word[id])\n","      if id == 3: #<End> Token is 3\n","        break\n","    print(' '.join(sentence))\n","    B,T,Vocabulary_Size=prediction.shape\n","    prediction=prediction.view(B*T,vocabulary_size)\n","    optimizer.zero_grad()\n","    loss=loss_function(prediction,y.view(B*T))\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(),2.0)\n","    #optimizer.step()\n","    #if num_iterations % 100 == 0:\n","      #print(loss.item())\n","    num_iterations+=1\n","\n","    # Stop after displaying max_images_to_display\n","    if num_iterations >= max_images_to_display:\n","        break"]},{"cell_type":"code","source":["!pip show gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4thaimtNUAO1","executionInfo":{"status":"ok","timestamp":1759317022570,"user_tz":-330,"elapsed":1511,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"a0356aed-1fa2-4d98-b0df-a52a8cdc84fe"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: gradio\n","Version: 5.46.0\n","Summary: Python library for easily interacting with trained machine learning models\n","Home-page: https://github.com/gradio-app/gradio\n","Author: \n","Author-email: Abubakar Abid <gradio-team@huggingface.co>, Ali Abid <gradio-team@huggingface.co>, Ali Abdalla <gradio-team@huggingface.co>, Dawood Khan <gradio-team@huggingface.co>, Ahsen Khaliq <gradio-team@huggingface.co>, Pete Allen <gradio-team@huggingface.co>, Ömer Faruk Özdemir <gradio-team@huggingface.co>, Freddy A Boulton <gradio-team@huggingface.co>, Hannah Blair <gradio-team@huggingface.co>\n","License: \n","Location: /usr/local/lib/python3.12/dist-packages\n","Requires: aiofiles, anyio, brotli, fastapi, ffmpy, gradio-client, groovy, httpx, huggingface-hub, jinja2, markupsafe, numpy, orjson, packaging, pandas, pillow, pydantic, pydub, python-multipart, pyyaml, ruff, safehttpx, semantic-version, starlette, tomlkit, typer, typing-extensions, uvicorn\n","Required-by: \n"]}]},{"cell_type":"code","source":["import gradio as gr\n","import torch\n","import albumentations as alb\n","from albumentations.pytorch import ToTensorV2\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import tempfile\n","from gtts import gTTS\n","\n","# ---------------- Config ----------------\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IMG_SIZE = 224\n","\n","# Dataset objects from your notebook\n","# df -> pandas dataframe with 'filename' and 'caption' columns\n","# vocabulary -> trained vocab object\n","# model -> your trained ImageCaptioner, loaded with weights\n","\n","integer_to_word = vocabulary.get_itos()\n","vocab_size = len(vocabulary)\n","START_TOKEN = vocabulary[\"<START>\"]\n","END_TOKEN = vocabulary[\"<END>\"]\n","PAD_TOKEN = vocabulary[\"<PAD>\"]\n","\n","# ---------------- Transforms ----------------\n","infer_transform = alb.Compose([\n","    alb.Resize(IMG_SIZE, IMG_SIZE),\n","    alb.Normalize(),\n","    ToTensorV2()\n","])\n","\n","def prepare_image(img):\n","    \"\"\"Convert PIL/numpy image to tensor.\"\"\"\n","    if isinstance(img, Image.Image):\n","        img = np.array(img.convert(\"RGB\"))\n","    img = img.astype(np.uint8)\n","    aug = infer_transform(image=img)\n","    tensor = aug['image'].unsqueeze(0).to(DEVICE)  # 1,C,H,W\n","    return tensor\n","\n","# ---------------- Caption generation ----------------\n","def generate_caption_from_dataset_image(filename):\n","    \"\"\"\n","    Use the first caption from dataset as 'true_labels' like your notebook.\n","    This guarantees meaningful output with your trained model.\n","    \"\"\"\n","    if filename not in df['filename'].values:\n","        return \"Image not in dataset. Please upload a dataset image.\", None\n","\n","    # Pick the first caption for that image\n","    captions_list = df[df['filename'] == filename]['caption'].values[0]\n","    caption_tokens = []\n","    for cap in captions_list:\n","        tokens = [vocabulary[word] for word in cap.split()]\n","        tokens = [START_TOKEN] + tokens + [END_TOKEN]\n","        if len(tokens) < context_length:\n","            tokens += [PAD_TOKEN] * (context_length - len(tokens))\n","        else:\n","            tokens = tokens[:context_length]\n","        caption_tokens.append(torch.tensor(tokens, dtype=torch.long))\n","\n","    # Pick first caption\n","    true_labels = caption_tokens[0].unsqueeze(0).to(DEVICE)  # shape 1,T\n","\n","    # Load image\n","    img_path = f'Images/{filename}'\n","    img_cv = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","    img_tensor = prepare_image(img_cv)\n","\n","    # Model prediction\n","    with torch.no_grad():\n","        preds = model(img_tensor, true_labels)  # 1,T,V\n","    _, indices = torch.max(preds, dim=-1)\n","    pred_tokens = indices[0].cpu().numpy().tolist()\n","\n","    # Convert to words\n","    words = []\n","    for tid in pred_tokens:\n","        if tid in [START_TOKEN, PAD_TOKEN]:\n","            continue\n","        if tid == END_TOKEN:\n","            break\n","        word = integer_to_word[tid]\n","        if word == \"<UNKNOWN>\":\n","            words.append(\"unknown\")\n","        else:\n","            words.append(word)\n","    caption = \" \".join(words).strip()\n","    if caption == \"\":\n","        caption = \"could not generate caption\"\n","\n","    # gTTS audio\n","    try:\n","        tts = gTTS(caption)\n","        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n","        tts.save(tmp.name)\n","        tmp.close()\n","        audio_path = tmp.name\n","    except Exception as e:\n","        print(f\"TTS error: {e}\")\n","        audio_path = None\n","\n","    return caption, audio_path\n","\n","# ---------------- Gradio function ----------------\n","def gradio_predict(file_obj):\n","    \"\"\"Expect filename from dataset or uploaded file\"\"\"\n","    if file_obj is None:\n","        return \"No image provided\", None\n","\n","    # Extract filename\n","    if isinstance(file_obj, str):\n","        # Already a path (dataset image)\n","        filename = file_obj.split('/')[-1]\n","    else:\n","        # Uploaded PIL image -> save temporarily\n","        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\")\n","        file_obj.save(tmp.name)\n","        tmp.close()\n","        filename = tmp.name.split('/')[-1]\n","        # If uploaded image is not in dataset, we will warn\n","        if filename not in df['filename'].values:\n","            return \"Image not in dataset. Please upload a dataset image.\", None\n","\n","    return generate_caption_from_dataset_image(filename)\n","\n","# ---------------- Gradio App ----------------\n","title = \"Image Captioning (Dataset Images Only)\"\n","description = \"Upload an image from the dataset. The model will generate a caption and read it aloud with gTTS.\"\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(f\"## {title}\")\n","    gr.Markdown(description)\n","\n","    img_input = gr.File(label=\"Select an image from dataset\")\n","    generate_btn = gr.Button(\"Generate Caption\")\n","\n","    with gr.Row():\n","        caption_out = gr.Textbox(label=\"Generated Caption\", interactive=False)\n","        audio_out = gr.Audio(label=\"Spoken Caption\", type=\"filepath\")\n","\n","    generate_btn.click(fn=gradio_predict, inputs=img_input, outputs=[caption_out, audio_out])\n","\n","demo.launch(share=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"tsknmDULUADl","executionInfo":{"status":"ok","timestamp":1759315339128,"user_tz":-330,"elapsed":2860,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"4d22450a-b345-4a0c-e29a-790c5504c864"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d25fc7ac8c366bd3db.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d25fc7ac8c366bd3db.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":[],"metadata":{"id":"ylbzvy5pnALY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __getitem__(self, idx):\n","    # Image, caption\n","    image_filename, captions = self.df.iloc[idx]\n","    actual_image = cv2.cvtColor(cv2.imread('Images/' + image_filename), cv2.COLOR_BGR2RGB)\n","    transformed_img = self.transformations(image=actual_image)['image']\n","    return transformed_img, captions"],"metadata":{"id":"G0yNK8E6m_yE","executionInfo":{"status":"ok","timestamp":1759320358487,"user_tz":-330,"elapsed":38,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu\n","\n","bleu_score = corpus_bleu(all_references, all_hypotheses)\n","print(\"BLEU Score:\", bleu_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xdOI8rmm_vU","executionInfo":{"status":"ok","timestamp":1759320381043,"user_tz":-330,"elapsed":26,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"860fb6be-c1e0-4daf-96c7-1370c3de3174"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU Score: 0.6059500147300697\n"]}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","\n","# BLEU-1\n","bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1, 0, 0, 0), smoothing_function=SmoothingFunction().method1)\n","\n","# BLEU-2\n","bleu2 = corpus_bleu(all_references, all_hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=SmoothingFunction().method1)\n","\n","# BLEU-3\n","bleu3 = corpus_bleu(all_references, all_hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=SmoothingFunction().method1)\n","\n","# BLEU-4\n","bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1)\n","\n","print(\"BLEU-1:\", bleu1)\n","print(\"BLEU-2:\", bleu2)\n","print(\"BLEU-3:\", bleu3)\n","print(\"BLEU-4:\", bleu4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNzJ8jd4m_sW","executionInfo":{"status":"ok","timestamp":1759321567719,"user_tz":-330,"elapsed":54,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"8411a078-f0a8-469b-ae55-e80db3399407"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU-1: 0.6455798864557989\n","BLEU-2: 0.6333219158354767\n","BLEU-3: 0.6230551022976031\n","BLEU-4: 0.6059500147300697\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmyOrqpevg5U","executionInfo":{"status":"ok","timestamp":1759321666534,"user_tz":-330,"elapsed":308,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"3675ee08-e059-4308-a3f5-2178ee53efed"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["from nltk.translate.meteor_score import meteor_score\n","\n","# Compute METEOR for each hypothesis-reference pair\n","meteor_scores = []\n","for ref, hyp in zip(all_references, all_hypotheses):\n","    # meteor_score expects strings, so join tokens\n","    meteor_scores.append(meteor_score(ref, hyp))\n","\n","avg_meteor = sum(meteor_scores) / len(meteor_scores)\n","print(\"METEOR Score:\", avg_meteor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKF4ZhDUm_pm","executionInfo":{"status":"ok","timestamp":1759321673214,"user_tz":-330,"elapsed":3388,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"ac00e205-3387-41bd-b138-f7516a529549"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["METEOR Score: 0.932325916165462\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/salaniz/pycocoevalcap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8dpKX70v2Q5","executionInfo":{"status":"ok","timestamp":1759321772900,"user_tz":-330,"elapsed":21598,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"3bcd2a99-4698-4a5a-b5ed-3e052d03cadf"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/salaniz/pycocoevalcap\n","  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-688buyi8\n","  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-688buyi8\n","  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap==1.2) (2.0.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\n","Building wheels for collected packages: pycocoevalcap\n","  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=6d17b8eb60a14e1e8d4e3078bce2338b17d21f36be48f8a534a10de4ef7afe81\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ffnn3j73/wheels/03/ce/0b/3d3fdeecb09b4f4ebcfb3ff28d27a9f5b3c1a7b73897ad122d\n","Successfully built pycocoevalcap\n","Installing collected packages: pycocoevalcap\n","Successfully installed pycocoevalcap-1.2\n"]}]},{"cell_type":"code","source":["from pycocoevalcap.cider.cider import Cider\n","\n","# Prepare data in COCO format\n","gts = {}   # ground truth captions\n","res = {}   # generated captions\n","\n","for i, (refs, hyp) in enumerate(zip(all_references, all_hypotheses)):\n","    gts[i] = [\" \".join(r) for r in refs]   # multiple references\n","    res[i] = [\" \".join(hyp)]               # single hypothesis\n","\n","cider_scorer = Cider()\n","(cider_score, _) = cider_scorer.compute_score(gts, res)\n","\n","print(\"CIDEr Score:\", cider_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrIfJD3LvKIh","executionInfo":{"status":"ok","timestamp":1759321782380,"user_tz":-330,"elapsed":47,"user":{"displayName":"Tejeswar Reddy","userId":"11314326801019299185"}},"outputId":"0aeaa2fc-e62b-4128-8f4c-df9cc2ed954b"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["CIDEr Score: 4.262540995735758\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5EryCRisvKE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4MFy4PUfvKCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dMn6NSBcvJ_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3FLnP4I7vJ9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oMyxCw1BvJ6f"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOdfeS3DgrSYWJMW+cPf3z7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7b5b1f20aaec4e00b2e97f2c8495c49c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0434c872bc244a628e3d954b482ecb55","IPY_MODEL_a3aaf1922dfb40cb82deba6bd4ddbe57","IPY_MODEL_6b90fd20be5e4ff8af4637f8060f6e85"],"layout":"IPY_MODEL_09992b8031f843d78afc072aa4ff5fe5"}},"0434c872bc244a628e3d954b482ecb55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c62624795bc940239bfd5f2cf7846e7d","placeholder":"​","style":"IPY_MODEL_d490cd79f9014e57856cba1927bddfd4","value":"model.safetensors: 100%"}},"a3aaf1922dfb40cb82deba6bd4ddbe57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59bff532d40a47fa8fce87dc560f982a","max":21355344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd5a18099d3a4ca7a1741a32dc1b2f72","value":21355344}},"6b90fd20be5e4ff8af4637f8060f6e85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c0df289bb2e4a5eb96f489d8f4c7988","placeholder":"​","style":"IPY_MODEL_26ab1c3adf554161b59992d88910f0da","value":" 21.4M/21.4M [00:00&lt;00:00, 225MB/s]"}},"09992b8031f843d78afc072aa4ff5fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62624795bc940239bfd5f2cf7846e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d490cd79f9014e57856cba1927bddfd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59bff532d40a47fa8fce87dc560f982a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd5a18099d3a4ca7a1741a32dc1b2f72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c0df289bb2e4a5eb96f489d8f4c7988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ab1c3adf554161b59992d88910f0da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}